{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType, ShortType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from spark_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.remote(\"sc://localhost\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Remote client cannot create a SparkContext. Create SparkSession instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\context.py:183\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    168\u001b[0m     master: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m     memory_profiler_cls: Type[MemoryProfiler] \u001b[38;5;241m=\u001b[39m MemoryProfiler,\n\u001b[0;32m    181\u001b[0m ):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_REMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_LOCAL_REMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m--> 183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote client cannot create a SparkContext. Create SparkSession instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m         )\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.allowSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;66;03m# In order to prevent SparkContext from being created in executors.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_assert_on_driver()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Remote client cannot create a SparkContext. Create SparkSession instead."
     ]
    }
   ],
   "source": [
    "sc = SparkContext().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000021B645B2D10>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession(sc).builder \\\n",
    "    .appName(\"dica33\") \\\n",
    "    .getOrCreate()\n",
    "    #.master(\"local[1]\") \\\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformations for dica33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {'Alimentazione': \"Scienza dell'alimentazione\",\n",
    " 'Allergie': 'Allergologia e immunologia',\n",
    " 'Apparato respiratorio': 'Pneumologia',\n",
    " 'Bocca e denti': 'Ortodonzia',\n",
    " 'Chirurgia estetica': 'Chirurgia plastica e ricostruttiva',\n",
    " 'Cuore circolazione e malattie del sangue': 'Cardiologia',\n",
    " 'Diabete tiroide e ghiandole': 'Endocrinologia',\n",
    " 'Farmaci e cure': 'Farmacologia',\n",
    " 'Fegato': 'Epatologia',\n",
    " 'Infanzia': 'Pediatria',\n",
    " 'Malattie infettive': 'Malattie infettive',\n",
    " 'Mente e cervello': 'Psicologia',\n",
    " 'Occhio e vista': 'Oculistica',\n",
    " 'Orecchie naso e gola': 'Otorinolaringoiatria',\n",
    " 'Pelle': 'Dermatologia e venereologia',\n",
    " 'Rene e vie urinarie': 'Urologia',\n",
    " 'Salute femminile': 'Ginecologia e ostetricia',\n",
    " 'Salute maschile': 'Andrologia',\n",
    " 'Scheletro e Articolazioni': 'Ortopedia',\n",
    " 'SessualitÃ ': 'Psicologia',\n",
    " 'Stomaco e intestino': 'Gastroenterologia e endoscopia digestiva',\n",
    " 'Tumori': 'Oncologia medica'}\n",
    "\n",
    "months_map = {\n",
    "    \"gennaio\": \"01\",\n",
    "    \"febbraio\": \"02\",\n",
    "    \"marzo\": \"03\",\n",
    "    \"aprile\": \"04\",\n",
    "    \"maggio\": \"05\",\n",
    "    \"giugno\": \"06\",\n",
    "    \"luglio\": \"07\",\n",
    "    \"agosto\": \"08\",\n",
    "    \"settembre\": \"09\",\n",
    "    \"ottobre\": \"10\",\n",
    "    \"novembre\": \"11\",\n",
    "    \"dicembre\": \"12\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_iso(date_str):\n",
    "    try:\n",
    "        parts = date_str.split()\n",
    "        day = parts[0]\n",
    "        month = months_map[parts[1].lower()]\n",
    "        year = parts[2]\n",
    "        date_iso = f\"{year}-{month}-{day}T00:00:00Z\"\n",
    "        return date_iso\n",
    "    except:\n",
    "        return \"0000-00-00T00:00:00Z\"\n",
    "    \n",
    "\n",
    "def split_dataframe(df):\n",
    "    df_rag = df.select(['URL', 'Category', 'Question' , 'Answer'])\n",
    "    df_doctors = df.select(['Doctor profile', 'Location'])\n",
    "    df_analytics = df.select(['URL', 'Category', 'Answer Date', 'Question Date'])\n",
    "\n",
    "    return df_rag, df_doctors, df_analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported from spark_functions.py\n",
    "chunking_udf = udf(lambda x: chunking(x, 300), ArrayType(StringType()))\n",
    "embeddings_udf = udf(lambda x: embed(x), ArrayType(FloatType()))\n",
    "locations_udf = udf(lambda x: get_coordinates(x, \"IT\"), ArrayType(FloatType()))\n",
    "\n",
    "mapping_udf = udf(lambda x: map_category(x, category_mapping), StringType())\n",
    "convert_to_iso_udf = udf(lambda x: convert_to_iso(x), StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations generali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dica33/data loaded\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/opt/spark/dica33/data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dica33_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../dica33/data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dica33_df \u001b[38;5;241m=\u001b[39m dica33_df\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m dica33_df \u001b[38;5;241m=\u001b[39m dica33_df\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\39392\\Desktop\\uni\\BigData\\RAG\\Spark\\Spark\\spark_functions.py:14\u001b[0m, in \u001b[0;36mload_dataframe\u001b[1;34m(spark, path)\u001b[0m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[0;32m      8\u001b[0m                     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      9\u001b[0m                     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     10\u001b[0m                     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     11\u001b[0m                     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mescape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m                     \u001b[38;5;241m.\u001b[39mcsv(path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprintSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1418\u001b[0m, in \u001b[0;36mDataFrame.printSchema\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprintSchema\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1413\u001b[0m, in \u001b[0;36mDataFrame._tree_string\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot analyze on empty plan.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1412\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[1;32m-> 1413\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtree_string\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\connect\\client.py:887\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[1;34m(self, method, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid state during retry exception handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m--> 887\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\connect\\client.py:1055\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;124;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[1;32m-> 1055\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[1;32mc:\\Users\\39392\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\connect\\client.py:1091\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[1;34m(self, rpc_error)\u001b[0m\n\u001b[0;32m   1089\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n\u001b[0;32m   1090\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[1;32m-> 1091\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/opt/spark/dica33/data."
     ]
    }
   ],
   "source": [
    "dica33_df = load_dataframe(spark, '../dica33/data')\n",
    "dica33_df = dica33_df.dropDuplicates(['URL'])\n",
    "dica33_df = dica33_df.dropDuplicates(['Question'])\n",
    "#mapping della categoria di dica33 sulle categorie predefinite\n",
    "dica33_df = dica33_df.withColumn('Category', mapping_udf(dica33_df['Category']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitta le informazioni per ogni database\n",
    "df_rag, df_doctors, df_analytics = split_dataframe(dica33_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations di df_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag = df_rag.where((length(col('Question')) > 30) & (length(col('Answer')) > 30))\n",
    "df_rag = df_rag.na.drop(how='any', subset=['Question', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag = df_rag.withColumn('Question', chunking_udf(df_rag['Question']))\n",
    "df_rag = df_rag.select('*', posexplode('Question').alias('Chunk_number', 'Chunked_Question'))\n",
    "df_rag = df_rag.drop('Question')\n",
    "df_rag = df_rag.withColumnRenamed('Chunked_Question', 'Question')\n",
    "#df_rag = df_rag.withColumn('embeddings', embeddings_udf(df_rag['Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations di df_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytics = df_analytics.where(~(col('Category').contains('MISSING')))\n",
    "df_analytics = df_analytics.na.drop(how='any', subset=['Category'])\n",
    "df_analytics = df_analytics.withColumn('Question Date', convert_to_iso_udf(df_analytics['Question Date']))\n",
    "df_analytics = df_analytics.withColumn('Answer Date', convert_to_iso_udf(df_analytics['Answer Date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations di df_doctors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors = df_doctors.dropDuplicates(['Doctor profile'])\n",
    "df_doctors = df_doctors.na.drop(how='any', subset=['Doctor profile', 'Location'])\n",
    "df_doctors = df_doctors.groupby(lower(col('location')).alias('location')).agg(collect_list(col(\"Doctor profile\")).alias('Doctor profile'))\n",
    "df_doctors = df_doctors.withColumn('coordinates', locations_udf(df_doctors['location']))\n",
    "df_doctors = df_doctors.where(col('coordinates').getItem(0) != 0.0)\n",
    "df_doctors = df_doctors.withColumn('Doctor profile', explode('Doctor profile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctors = df_doctors.dropDuplicates(['Doctor profile'])\n",
    "df_doctors = df_doctors.na.drop(how='any', subset=['Doctor profile', 'Location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag.write.json('../dica33/json_dica33/rag', mode=\"overwrite\")\n",
    "df_analytics.write.json('../dica33/json_dica33/analytics', mode=\"overwrite\")\n",
    "df_doctors.write.json('../dica33/json_dica33/doctors', mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
